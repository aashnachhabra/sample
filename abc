import logging
import os
import io
import allure
import pandas as pd
import pytest
from tests.vikings.apps.becca.config.Constants import Constants
from tests.vikings.fw.common.FW import FW
from tests.vikings.apps.becca.ui_pages.LoginPage import LoginPage
from tests.vikings.fw.common.CommonUtils import CommonUtils

@pytest.fixture(scope="function")
def rows_accumulator(request):
    rows_dict = {}
    yield rows_dict

@pytest.fixture(scope="function", autouse=True)
def generate_result(request,rows_accumulator, tmp_path_factory):
    yield
    COLUMNS_ORDER = getattr(request.node, "COLUMNS_ORDER", [])
    if COLUMNS_ORDER:
        temp_dir = tmp_path_factory.getbasetemp()
        temp_file = temp_dir / f"{FW.get_result_file_name(os.getpid())}"
        logging.info(f"base temp:{temp_dir}")
        logging.info(f"file path:{temp_file}")
        if rows_accumulator:
            df = pd.DataFrame(list(rows_accumulator.values()))
            for col in COLUMNS_ORDER:
                if col not in df.columns:
                    df[col] = ""
            df = df[COLUMNS_ORDER]
            if temp_file.exists():
                df.to_csv(temp_file, mode='a', header=False, index=False)
            else:
                df.to_csv(temp_file, index=False)
    else:
        logging.info("No COLUMNS_ORDER found on the test node.")

@pytest.fixture(scope="function", autouse=True)
def test_summary(request, rows_accumulator):
    yield
    try:
        COLUMNS_ORDER = getattr(request.node, "COLUMNS_ORDER", [])
        if COLUMNS_ORDER :
            if rows_accumulator:
                df = pd.DataFrame(list(rows_accumulator.values()))
                for col in COLUMNS_ORDER:
                    if col not in df.columns:
                        df[col] = ""
                df = df[COLUMNS_ORDER]
                csv_buffer = io.StringIO()
                df.to_csv(csv_buffer, index=False)
                allure.attach(csv_buffer.getvalue(), name="Test summary", attachment_type=allure.attachment_type.CSV)
        else:
            logging.info("No COLUMNS_ORDER found on the test node for summary attachment.")
    except Exception as e:
        logging.info(f"CSV Attach Error: {e}")

@pytest.fixture(scope="session")
def authenticated_context(browser):
    state_path = Constants.BECCA_STORAGE_STATE_PATH

    if not os.path.exists(state_path):
        raise RuntimeError(
            f"Storage state not found at {state_path}. "
            f"Run: python -m tests.vikings.apps.becca.auth.generate_storage_state"
        )

    # Create ONE persistent context for all tests
    context = browser.new_context(storage_state=state_path)

    # Warm up session (ensures Azure login is bypassed)
    page = context.new_page()
    page.goto(Constants.BECCA_URL, wait_until="networkidle")

    yield context
    context.close()

@pytest.fixture()
def becca_page(browser):
    is_local = CommonUtils.is_local_run()
    state_path = Constants.BECCA_STORAGE_STATE_PATH
    if is_local:
        if not os.path.exists(state_path):
            raise RuntimeError(
                f"[LOCAL RUN] Storage state not found at {state_path}.\n"
                "Generate it using:\n"
                "  python -m tests.vikings.apps.becca.auth.generate_storage_state\n"
            )

        logging.info(f"[BECCA AUTH] Local run detected -> using storageState: {state_path}")
        context = browser.new_context(storage_state=state_path)
        page = context.new_page()
        page.goto(Constants.BECCA_URL, wait_until="domcontentloaded")
        yield page
        context.close()

    else:
        logging.info("[BECCA AUTH] Non-local run detected -> using LoginPage.login()")
        context = browser.new_context()
        page = context.new_page()
        login = LoginPage(page)
        login.login()
        yield page
        context.close()

import os
from playwright.sync_api import sync_playwright, expect
from tests.vikings.apps.becca.config.Constants import Constants
from tests.vikings.fw.common.FW import FW
from tests.vikings.fw.common.Results import Results
import re
from dotenv import load_dotenv
load_dotenv(".env.local")

AZURE_USERNAME_INPUT = 'input[name="loginfmt"]'
AZURE_PASSWORD_INPUT = 'input[name="passwd"]'
AZURE_SUBMIT_BUTTON = 'input[type="submit"]'
AZURE_STAY_SIGNED_IN_YES = 'input[value="Yes"]'


def generate_storage_state():
    username = FW.get_env("BECCA_APP_CIRRUS_UI_USERNAME")
    password = FW.get_env("BECCA_APP_CIRRUS_UI_PASSWORD")

    if not username or not password:
        raise RuntimeError("Missing BECCA_APP_CIRRUS_UI_USERNAME or BECCA_APP_CIRRUS_UI_PASSWORD")

    state_path = Constants.BECCA_STORAGE_STATE_PATH
    os.makedirs(os.path.dirname(state_path), exist_ok=True)

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False, slow_mo=200)
        context = browser.new_context()
        page = context.new_page()

        page.goto(Constants.BECCA_URL, wait_until="domcontentloaded")

        page.wait_for_selector(AZURE_USERNAME_INPUT, timeout=30000)
        page.fill(AZURE_USERNAME_INPUT, username)
        page.click(AZURE_SUBMIT_BUTTON)

        page.wait_for_selector(AZURE_PASSWORD_INPUT, timeout=30000)
        page.fill(AZURE_PASSWORD_INPUT, password)
        page.click(AZURE_SUBMIT_BUTTON)

        try:
            page.wait_for_selector(AZURE_STAY_SIGNED_IN_YES, timeout=5000)
            page.click(AZURE_STAY_SIGNED_IN_YES)
        except Exception:
            pass

        print("\n‚úÖ Complete MFA in the opened browser window.")
        print("‚úÖ Once BECCA loads, come back and press Resume in the Playwright Inspector.\n")

        page.pause()

        page.wait_for_load_state("networkidle", timeout=60000)
        expect(page).not_to_have_url(re.compile(r".*login\.microsoftonline\.com.*"))

        context.storage_state(path=state_path)
        Results.report_event(f"Saved storage state to: {state_path}")
        print(f"\n‚úÖ storageState saved at: {state_path}\n")

        context.close()
        browser.close()


if __name__ == "__main__":
    generate_storage_state()

import logging
import traceback
import time
import re
from typing import Dict, Any, Tuple

import allure
from tests.vikings.conftest import THRESHOLD
from tests.vikings.fw.common.Results import Results
from tests.vikings.fw.ui.BasePage import BasePage
from tests.vikings.fw.utils import get_multiple_scores


class BeccaPage(BasePage):
    CHAT_INPUT_SELECTOR = ".input-section textarea[matInput]"
    SUBMIT_BUTTON_SELECTOR = "button.submit-button"
    START_NEW_CHAT_SELECTOR = "button:has-text('Start New Chat')"
    CHAT_MESSAGE_SELECTOR = "div.chat-box app-chat-message"

    GENERATION_TIMEOUT_SECS = 180

    def __init__(self, page):
        self.page = page

    def open_becca(self):
        with allure.step("Open Becca UI"):
            self.page.goto(self.page.url, wait_until="domcontentloaded")

    def start_new_chat_if_present(self):
        """
        Angular-safe Start New Chat:
        - Button is conditional
        - Messages clear asynchronously
        """
        btn = self.page.locator(self.START_NEW_CHAT_SELECTOR)

        if btn.count() == 0:
            return

        try:
            btn.first.wait_for(state="visible", timeout=3000)
            btn.first.click()

            self.page.wait_for_function(
                "document.querySelectorAll('app-chat-message').length === 0",
                timeout=5000
            )
        except Exception:
            pass

    def _wait_for_generation_complete(self):
        """
        Waits for Submit -> Cancel -> Submit cycle.
        This is the ONLY reliable signal that generation finished.
        """
        self.page.wait_for_function(
            """() => {
                const btn = document.querySelector('button.submit-button');
                return btn && btn.innerText.trim() === 'Submit';
            }""",
            timeout=self.GENERATION_TIMEOUT_SECS * 1000
        )

    def _get_stable_latest_message(self, timeout_secs: int = 5) -> str:
        """
        Waits until the streamed response stops changing.
        """
        last = self.page.locator(self.CHAT_MESSAGE_SELECTOR).last
        last.wait_for(state="visible", timeout=30000)

        previous = ""
        end = time.time() + timeout_secs

        while time.time() < end:
            current = last.inner_text().strip()
            if current == previous:
                return current
            previous = current
            time.sleep(0.5)

        return previous

    def _strip_ui_chrome(self, text: str) -> str:
        noise = ["content_copy", "star_border", "thumb_up_off_alt", "thumb_down_off_alt"]
        for n in noise:
            text = text.replace(n, "")
        return text.strip()


    def _format_prompt_with_testcase(self, prompt: str, testcase: Dict[str, Any]) -> str:
        """
        Replaces $ColumnName placeholders using testcase values.
        Example: $PlanCode -> DY82
        """
        if not prompt:
            return ""

        def replacer(match):
            key = match.group(1)
            for col, val in testcase.items():
                if col.lower() == key.lower() and str(val).strip():
                    return str(val).strip()
            return match.group(0)

        return re.sub(r"\$(\w+)", replacer, prompt).strip()

    def _normalize(self, text: str) -> str:
        if not text:
            return ""
        text = text.lower()
        text = re.sub(r"[^\w\s%$.-]", " ", text)
        text = re.sub(r"\s+", " ", text)
        return text.strip()


    def send_prompt_and_wait(self, prompt: str) -> str:
        self.start_new_chat_if_present()

        input_box = self.page.locator(self.CHAT_INPUT_SELECTOR).first
        submit_btn = self.page.locator(self.SUBMIT_BUTTON_SELECTOR).first

        input_box.wait_for(state="visible", timeout=30000)

        input_box.fill("")
        input_box.fill(prompt)

        submit_btn.wait_for(state="visible", timeout=30000)
        submit_btn.click()

        self._wait_for_generation_complete()

        response = self._get_stable_latest_message()
        self.attach_screenshot("Becca response")

        return self._strip_ui_chrome(response)

    def run_prompt_and_get_response(self, prompt: str) -> Tuple[str, float]:
        start = time.time()
        response = self.send_prompt_and_wait(prompt)
        return response, round(time.time() - start, 2)


    def _validate_response(
            self,
            actual_response: str,
            expected_answer: str,
            validation_text: str,
            testcase: Dict[str, Any],
    ) -> Tuple[str, str]:

        if not actual_response or self._normalize(actual_response) in {"", "none"}:
            return "failed", "ACTUAL_RESPONSE is empty"

        actual_norm = self._normalize(actual_response)

        if validation_text:
            tokens = [t.strip() for t in validation_text.split(",") if t.strip()]

            col_map = {
                k.lower(): str(v).strip()
                for k, v in testcase.items()
                if str(v).strip()
            }

            resolved_tokens = [
                col_map.get(tok.lower(), tok) for tok in tokens
            ]

            missing = [
                tok for tok in resolved_tokens
                if self._normalize(tok) not in actual_norm
            ]

            if missing:
                return "failed", f"Missing tokens in ACTUAL_RESPONSE: {missing}"

            return "passed", ""

        if expected_answer:
            expected_norm = self._normalize(expected_answer)
            if expected_norm and expected_norm in actual_norm:
                return "passed", ""
            return "failed", "Expected response not found in actual response"

        return "passed", ""


    def execute_testcase(self, testcase: Dict[str, Any], rows_accumulator: Dict[str, Any]):
        with allure.step("Ask Question in Becca"):
            raw_prompt = str(testcase.get("prompt", "")).strip()
            expected_answer = str(testcase.get("expected_answer", "")).strip()
            validation_text = str(testcase.get("validation_text", "")).strip()

            formatted_prompt = self._format_prompt_with_testcase(raw_prompt, testcase)

            row_key = testcase.get("id") or formatted_prompt

            try:
                actual_response, time_taken = self.run_prompt_and_get_response(formatted_prompt)

                ui_status, ui_error = self._validate_response(
                    actual_response=actual_response,
                    expected_answer=expected_answer,
                    validation_text=validation_text,
                    testcase=testcase,
                )

                ui_row = {
                    **testcase,
                    "actual_prompt": formatted_prompt,
                    "expected_response": expected_answer,
                    "actual_response": actual_response,
                    "ui_status": ui_status,
                    "ui_error": ui_error,
                    "Response time(secs)": time_taken,
                }

            except Exception as e:
                logging.error(e)
                allure.attach(traceback.format_exc(), "UI exception", allure.attachment_type.TEXT)
                ui_row = {
                    **testcase,
                    "actual_prompt": formatted_prompt,
                    "actual_response": "error while fetching ui response",
                    "ui_status": "failed",
                    "ui_error": str(e),
                    "Response time(secs)": 0,
                }

            rows_accumulator[row_key] = ui_row

            Results.report_event_assert_bool(
                ui_row["ui_status"] == "passed",
                f"prompt failed: {formatted_prompt} | {ui_row['ui_error']}",
                )

            return ui_row

    def evaluate_testcase(self, ui_row: Dict[str, Any], rows_accumulator: Dict[str, Any]):
        with allure.step("Evaluate Becca Testcase"):

            if ui_row["ui_status"] != "passed":
                ui_row.update({
                    "eval_status": "skipped",
                    "eval_error": "UI validation failed",
                    "eval_threshold": "",
                    "gEval_score": "",
                    "gEval_reason": "",
                })
                return ui_row

            try:
                threshold = THRESHOLD
                expected = ui_row.get("expected_response","").strip()
                payload = {
                    "question": ui_row["actual_prompt"],
                    "llmAnswer": ui_row["actual_response"],
                    "groundAnswer": expected,
                    "contextualPrecision": False,
                    "contextualRecall": False,
                    "faithfulness": False,
                    "gEval": True,
                }

                data = get_multiple_scores(payload)
                score = data.get("gEval", {}).get("score", 0)
                reason = data.get("gEval", {}).get("reason", "")

                ui_row.update({
                    "eval_threshold": threshold,
                    "gEval_score": score,
                    "gEval_reason": reason,
                    "eval_status": "passed" if score >= threshold else "failed",
                    "eval_error": "",
                })

            except Exception as e:
                allure.attach(traceback.format_exc(), "Eval exception", allure.attachment_type.TEXT)
                ui_row.update({
                    "eval_status": "failed",
                    "eval_error": str(e),
                })

            return ui_row

import allure
import pytest

from settings import BECCA_XL
from tests.vikings.fw.common.CommonUtils import CommonUtils
from tests.vikings.fw.common.FW import FW

from ..ui_pages.BeccaPage import BeccaPage
from ..ui_pages.LogoutPage import LogoutPage


COLUMNS_ORDER = [
    "actual_prompt",
    "expected_response",
    "actual_response",
    "ui_status",
    "ui_error",
    "Response time(secs)",
    "eval_threshold",
    "gEval_score",
    "gEval_reason",
    "eval_status",
    "eval_error",
]
from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
import allure

def test_login_becca_helper(request, becca_page, testcase, rows_accumulator):
    request.node.COLUMNS_ORDER = COLUMNS_ORDER

    becca = BeccaPage(becca_page)
    logout = LogoutPage(becca_page)

    try:
        CommonUtils.setupTest(testcase)

        becca.open_becca()
        ui_row = becca.execute_testcase(testcase, rows_accumulator)

        if ui_row["ui_status"] == "passed":
            becca.evaluate_testcase(ui_row, rows_accumulator)

        try:
            logout.logout()
        except PlaywrightTimeoutError as e:
            allure.attach(
                str(e),
                name="Logout skipped (Tools not found)",
                attachment_type=allure.attachment_type.TEXT
            )
            becca.attach_screenshot("Logout skipped - Tools not found")

    except Exception as e:
        CommonUtils.handleException(testcase, rows_accumulator, e)
        raise

@pytest.mark.parametrize("testcase", FW.get_test_data(BECCA_XL))
@pytest.mark.uieval
@allure.feature("becca")
@allure.story("ui_eval_e2e")
def test_login_becca_logout_e2e(request, becca_page, testcase, rows_accumulator):
    test_login_becca_helper(request, becca_page, testcase, rows_accumulator)


@pytest.mark.parametrize("testcase", FW.get_test_data(BECCA_XL)[:1])
@pytest.mark.smoke
@allure.feature("becca")
@allure.story("ui_eval_e2e")
def test_login_becca_logout_smoke(request, becca_page, testcase, rows_accumulator):
    test_login_becca_helper(request, becca_page, testcase, rows_accumulator)
when i run the test with this command with parallel process its returna 429 too many requests and many test fail however in series it works fine
command im using
tests/vikings/apps/becca -m "uieval" --tracing off -s --alluredir allure-results --reruns 0 -n 10 --html=reports/report.html --self-contained-html

is there a way i can fix it minimally so that it works perfecty in -n 1 and -n 10 both cases my manager gave me this code ref go through and let me know if its helpful
except Exception as e:
        # Handle rate limits and other errors
        error_type = type(e).__name__
        error_msg = str(e)
 
        # Check for rate limit error (429)
        if "RateLimitError" in error_type or "429" in error_msg or "rate limit" in error_msg.lower():
            # Extract retry delay from error message (default to 5 seconds)
            import re
            delay_match = re.search(r'retry after (\d+)', error_msg)
            retry_delay = int(delay_match.group(1)) if delay_match else 5
 
            logging.warning(f"Rate limit exceeded. Waiting {retry_delay}s before continuing...")
            allure.attach(f"Rate limit hit. Retry after {retry_delay}s",
                          name="Rate Limit Warning",
                          attachment_type=allure.attachment_type.TEXT)
 
            # Wait for the suggested delay plus buffer
            time.sleep(retry_delay + REQUEST_BUFFER_SEC)
 
        if session_results:
            session_results["failed"] += 1
 
        csv_accumulator[question] = {
            "question": question,
            "ground_answer": ground_answer,
            "context": context,
            "LLM Answer": cody_response,
            "Deepeval test": "Failed",
            "gEval score": 0,
            "gEval reason": error_msg,
            "answer relevancy score": 0,
            "answer relevancy reason": "",
            "faithfulness score": 0,
            "faithfulness reason": "",
            "contextual precision score": 0,
            "contextual precision reason": "",
            "contextual recall score": 0,
            "contextual recall reason": "",
            "llm response time": llm_elapsed_ms
        }
 
        logging.info(f"Waiting {REQUEST_BUFFER_SEC}s before next request...")
        time.sleep(REQUEST_BUFFER_SEC)


import logging
import pytest
import requests
import pandas as pd
import json
import allure
import os
import time
import urllib3
from tests.vikings.fw.utils import get_multiple_scores
from settings import CODY_XL, COMMON_REPORTS_DIR
import re
import os
from tests.vikings.conftest import SSLV
from tests.vikings.conftest import THRESHOLD

# Force SSL verification to False - Override any conftest settings
# Disable SSL warnings globally
#urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
#logging.warning("SSL verification is DISABLED for Cody tests")

# Create a session with SSL disabled to ensure consistency

#SESSION.verify = False

import deepeval_utils.metrics.utils as deepeval_utils

def patched_trimAndLoadJson(input_string: str, metric=None):
    """Patched version with better escape handling"""
    start = input_string.find("{")
    end = input_string.rfind("}") + 1

    if end == 0 and start != -1:
        input_string = input_string + "}"
        end = len(input_string)

    jsonStr = input_string[start:end] if start != -1 and end != 0 else ""

    # Fix common escape issues
    jsonStr = jsonStr.replace("\\'", "'")  # Fix escaped single quotes
    jsonStr = re.sub(r'\\(?!["\\/bfnrtu])', r'\\\\', jsonStr)  # Escape unescaped backslashes
    jsonStr = re.sub(r",\s*([\]}])", r"\1", jsonStr)  # Remove trailing commas

    try:
        return json.loads(jsonStr)
    except json.JSONDecodeError as e:
        error_str = f"Evaluation LLM outputted invalid JSON: {e}"
        if metric is not None:
            metric.error = error_str
        raise ValueError(error_str)

# Apply monkey patch at module load
deepeval_utils.trimAndLoadJson = patched_trimAndLoadJson


def get_test_data():
    df = pd.read_excel(CODY_XL, engine='openpyxl')
    return df.to_dict('records')


@pytest.fixture(scope="session", autouse=True)
def summary_fixture(request):
    results = {"total": 0, "passed": 0, "failed": 0}
    request.session.results = results
    yield
    summary_html = f"""
    <table>
      <tr><th>Total Questions</th><td>{results['total']}</td></tr>
      <tr><th>Passed Questions</th><td>{results['passed']}</td></tr>
      <tr><th>Failed Questions</th><td>{results['failed']}</td></tr>
    </table>
    """
    allure.attach(summary_html, name="Summary Table", attachment_type=allure.attachment_type.HTML)


@pytest.fixture(scope="session")
def csv_accumulator(request):
    rows_dict = {}
    request.session.csv_rows_dict = rows_dict
    yield rows_dict


def _make_llm_request(request, endpoint: str, payload: dict, headers: dict, verify: SSLV,timeout: float = 120.0, max_retries: int = 10) -> dict:
    """
    Make LLM request with retry logic.
    Retries indefinitely on 504 Gateway Timeout with FIXED delay (no backoff).
    Uses exponential backoff only for other errors.
    """
    attempt = 0
    retry_delay_504 = float(os.getenv('RETRY_DELAY_504', '3.0'))

    while True:
        try:
            start_ts = time.perf_counter()

            # Use the session object with verify=False
            response = request.session.post(
                endpoint,
                json=payload,
                headers=headers,
                timeout=timeout,
                verify=verify  # Explicit SSL bypass
            )

            end_ts = time.perf_counter()
            elapsed_ms = (end_ts - start_ts) * 1000.0

            logging.info(f"LLM POST attempt {attempt + 1} completed in {elapsed_ms:.2f} ms (status={response.status_code})")

            # Check for 504 Gateway Timeout - retry indefinitely with FIXED delay
            if response.status_code == 504:
                logging.warning(f"üîÑ HTTP 504 Gateway Timeout on attempt {attempt + 1}. Retrying in {retry_delay_504}s...")
                print(f"‚ö†Ô∏è  HTTP 504 Gateway Timeout - Retry #{attempt + 1} after {retry_delay_504}s delay")
                time.sleep(retry_delay_504)  # FIXED delay - no exponential backoff
                attempt += 1
                continue  # Retry indefinitely

            # Raise for other HTTP errors
            response.raise_for_status()

            # Parse JSON response
            try:
                response_json = response.json()
            except json.JSONDecodeError as e:
                logging.warning(f"JSON decode failed: {e}. Using raw text.")
                response_json = {"response": response.text, "context": ""}

            print(f"‚úÖ LLM request succeeded after {attempt + 1} attempts")

            return {
                "status_code": response.status_code,
                "response": response_json.get('response', response.text),
                "context": response_json.get('context', ""),
                "error": None
            }

        except requests.exceptions.SSLError as e:
            logging.error(f"SSL Error on attempt {attempt + 1}: {e}")
            if attempt >= max_retries - 1:
                return {
                    "status_code": None,
                    "response": "sslError",
                    "context": f"SSL Certificate Error after {max_retries} attempts: {e}\nEndpoint: {endpoint}",
                    "error": str(e)
                }
            attempt += 1

        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
            delay = min(2 ** min(attempt, 6), 60)  # Exponential backoff for network errors
            logging.warning(f"Network error on attempt {attempt + 1}: {e}. Retrying in {delay}s...")
            print(f"‚ö†Ô∏è  Network error - Retry #{attempt + 1} after {delay}s delay")

            if attempt < max_retries - 1:
                time.sleep(delay)
                attempt += 1
            else:
                return {
                    "status_code": None,
                    "response": "networkError",
                    "context": f"Network Error after {max_retries} attempts: {type(e).__name__}\nError: {e}\nEndpoint: {endpoint}",
                    "error": str(e)
                }

        except requests.exceptions.HTTPError as e:
            status_code = response.status_code if 'response' in locals() else None

            # Retry indefinitely on 504 with FIXED delay
            if status_code == 504:
                logging.warning(f"üîÑ HTTP 504 Gateway Timeout on attempt {attempt + 1}. Retrying in {retry_delay_504}s...")
                print(f"‚ö†Ô∏è  HTTP 504 Gateway Timeout - Retry #{attempt + 1} after {retry_delay_504}s delay")
                time.sleep(retry_delay_504)  # FIXED delay
                attempt += 1
                continue  # Retry indefinitely

            # Exponential backoff for other 5xx errors
            if status_code and 500 <= status_code < 600:
                delay = min(2 ** min(attempt, 6), 60)
                logging.warning(f"HTTP {status_code} error on attempt {attempt + 1}. Retrying in {delay}s...")
                print(f"‚ö†Ô∏è  HTTP {status_code} error - Retry #{attempt + 1} after {delay}s delay")

                if attempt < max_retries - 1:
                    time.sleep(delay)
                    attempt += 1
                else:
                    return {
                        "status_code": status_code,
                        "response": "httpError",
                        "context": f"HTTP Error after {max_retries} attempts: {e}\nStatus: {status_code}",
                        "error": str(e)
                    }
            else:
                # Don't retry on 4xx client errors
                logging.error(f"HTTP Error: {e}")
                return {
                    "status_code": status_code,
                    "response": "httpError",
                    "context": f"HTTP Error: {e}\nStatus: {status_code}",
                    "error": str(e)
                }

        except Exception as e:
            logging.error(f"Unknown error on attempt {attempt + 1}: {type(e).__name__}: {e}")
            if attempt >= max_retries - 1:
                return {
                    "status_code": None,
                    "response": "runtimeError",
                    "context": f"Runtime Error after {max_retries} attempts: {type(e).__name__}\nError: {e}",
                    "error": str(e)
                }
            attempt += 1
# Add at the top of the file with other constants
REQUEST_BUFFER_SEC = float(os.getenv('REQUEST_BUFFER_SEC', '2.0'))  # 2 seconds default buffer

def _process_question(row, request, csv_accumulator, test_type):
    llm_endpoint = 'https://cody-dev.prod.internal-gcp.optum.com/api/external/contextandresponsebinderfortesteval'
    threshold = THRESHOLD

    session_results = getattr(request.session, "results", None)
    if session_results:
        session_results["total"] += 1

    allure.dynamic.title(row['question'])
    question = row['question']
    ground_answer = row['expected_answer']
    context=""

    allure.dynamic.label("type", test_type)

    with allure.step(f"{test_type.capitalize()} Test for question: {question}"):
        jwt_token = "eyJhbGciOiJSUzI1NiIsImtpZCI6IjhlVGozRDVDTVR5cGZubzZaQVhQMTJ5TUVBdyIsInBpLmF0bSI6Ijhkdm4ifQ.eyJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGFkZHJlc3MgZW1haWwgcGhvbmUiLCJjbGllbnRfaWQiOiJSZWcxRGV2X1BPQ19DT05GSUdfREVTS1RPUF9QTCIsInN0YW5kYXJkQVRNIjoiMG5mWEdLQ1FTZm12dVRHczU1eFVzTVB5dGZEQlVvY1ciLCJhY3IiOiJSMV9BQUwxX01TLUFELUtlcmJlcm9zIiwiYXVkIjoiUmVnMURldl9QT0NfQ09ORklHX0RFU0tUT1BfUEwiLCJzdWIiOiJhY2hoYWIxNiIsIm1zaWQiOiJhY2hoYWIxNiIsImlzcyI6Imh0dHBzOi8vYXV0aGdhdGV3YXkxLWRldi5lbnRpYW0udWhnLmNvbSIsImVtcGxveWVlSUQiOiIwMDIwNDM2NDUiLCJleHAiOjE2OTEwNTE1NTR9.X1tXnaZ6xFTrhJgSfq58z_C-tefSoKIbfC8m1VnFOYQrNdnYzlkNLykG-7ya794QWofnJ8_0L2CtrTg-oASRgP2j8ngNErRlhFRSjxtJMJYmqwx6WEXWf07FTUufEP9M5ZZpKPRMgCS-J0_cg_Dj9w6dqx9DDtzDGCPF1NOGXZM3TGf4HorFe01muieszujxttBRiMjSOFLFdumF_bG1FLUlYXostavBS7zxHc07YwRwi4gWznTqueIsg4XicWp8nbiegjVSWsQX2LGuO18upCQLEUZhO8-W7s1mopbk83Pz3fl1CXoXp77iYOMz3iheiZ42UyrFHYNgZYam4c8S4A"
        headers = {
            'Authorization': 'Bearer ' + jwt_token,
            'Cookie': 'JSESSIONID=BA1FF4A64A7D5A72AD6FA2F0864B6C78'
        }
        payload = {"text": question}

        llm_start_time = time.perf_counter()
        result = _make_llm_request(request, llm_endpoint, payload, headers, SSLV)
        llm_end_time = time.perf_counter()
        llm_elapsed_ms = (llm_end_time - llm_start_time) * 1000.0

        logging.info(f"LLM request completed in {llm_elapsed_ms:.2f} ms")

        cody_response = result["response"]
        context = result["context"]
        error = result["error"]

        # Attach to Allure
        allure.attach(question, name="Question", attachment_type=allure.attachment_type.TEXT)
        allure.attach(context or "No context available", name="Context", attachment_type=allure.attachment_type.TEXT)
        allure.attach(ground_answer, name="Expected Answer", attachment_type=allure.attachment_type.TEXT)
        allure.attach(cody_response or "No response", name="Actual Answer", attachment_type=allure.attachment_type.TEXT)
        allure.attach(f"LLM Response Time: {llm_elapsed_ms:.2f} ms",
                      name="LLM Response Timing",
                      attachment_type=allure.attachment_type.TEXT
                      )

        if error:
            allure.attach(error, name="Error Details", attachment_type=allure.attachment_type.TEXT)

    # Validate required data before evaluation
    if not cody_response or not context:
        logging.warning(f"Missing data - Response: {bool(cody_response)}, Context: {bool(context)}")
        if not cody_response:
            cody_response = "No response generated"
        if not context:
            context = "No context available"

    # Evaluation logic remains the same...
    eval_payload = {
        "question": question,
        "llmAnswer": cody_response,
        "groundAnswer": ground_answer,
        "retrievalContext": [context],
        "contextualPrecision": True,
        "contextualRecall": True,
        "faithfulness": True,
        "answerRelevancy": True,
        "gEval": True
    }

    try:
        data = get_multiple_scores(eval_payload)

        # Extract scores with defensive parsing
        gEval_score = data.get("gEval", {}).get("score", 0)
        gEval_reason = data.get("gEval", {}).get("reason", "")
        answer_relevancy_score = data.get("answerRelevancy", {}).get("score", 0)
        answer_relevancy_reason = data.get("answerRelevancy", {}).get("reason", "")
        faithfulness_score = data.get("faithfulness", {}).get("score", 0)
        faithfulness_reason = data.get("faithfulness", {}).get("reason", "")

        # Print to console immediately
        print(f" gEval Score: {gEval_score:.4f}")
        print(f"üìù gEval Reason: {gEval_reason[:200]}...\n")

        # Handle contextual precision parsing errors
        try:
            contextual_precision_score = data.get("contextualPrecision", {}).get("score", 0)
            contextual_precision_reason = data.get("contextualPrecision", {}).get("reason", "")
        except (TypeError, KeyError, AttributeError) as cp_error:
            logging.warning(f"Contextual precision parsing error: {cp_error}. Using default values.")
            contextual_precision_score = 0
            contextual_precision_reason = f"Parsing error: {str(cp_error)}"
        except ValueError as val_err:
            # Handle deepeval JSON parsing errors
            if "invalid JSON" in str(val_err).lower():
                error_msg = f"Deepeval LLM generated malformed JSON: {str(val_err)}"
                logging.error(f"{error_msg}. Skipping metric evaluation.")
            else:
                error_msg = f"Deepeval validation error: {str(val_err)}"

        # Handle contextual recall parsing errors
        try:
            contextual_recall_score = data.get("contextualRecall", {}).get("score", 0)
            contextual_recall_reason = data.get("contextualRecall", {}).get("reason", "")
        except (TypeError, KeyError, AttributeError) as cr_error:
            logging.warning(f"Contextual recall parsing error: {cr_error}. Using default values.")
            contextual_recall_score = 0
            contextual_recall_reason = f"Parsing error: {str(cr_error)}"

        result = "Passed" if all([
            gEval_score >= threshold,
            answer_relevancy_score >= threshold,
            faithfulness_score >= threshold,
            contextual_precision_score >= threshold,
            contextual_recall_score >= threshold
        ]) else "Failed"

        if session_results:
            if result == "Passed":
                session_results["passed"] += 1
            else:
                session_results["failed"] += 1

        csv_accumulator[question] = {
            "question": question,
            "ground_answer": ground_answer,
            "context": context,
            "LLM Answer": cody_response,
            "Deepeval test": result,
            "gEval score": gEval_score,
            "gEval reason": gEval_reason,
            "answer relevancy score": answer_relevancy_score,
            "answer relevancy reason": answer_relevancy_reason,
            "faithfulness score": faithfulness_score,
            "faithfulness reason": faithfulness_reason,
            "contextual precision score": contextual_precision_score,
            "contextual precision reason": contextual_precision_reason,
            "contextual recall score": contextual_recall_score,
            "contextual recall reason": contextual_recall_reason,
            "llm response time": llm_elapsed_ms
        }

        logging.info(f"Evaluation completed - Result: {result}, gEval: {gEval_score}, "
                     f"Relevancy: {answer_relevancy_score}, Faithfulness: {faithfulness_score}, "
                     f"Precision: {contextual_precision_score}, Recall: {contextual_recall_score}")

        # Add buffer time before next request
        logging.info(f"Waiting {REQUEST_BUFFER_SEC}s before next request...")
        time.sleep(REQUEST_BUFFER_SEC)

        if result != "Passed":
            pytest.fail(f"Question failed: {question}", pytrace=True)

    except (TypeError, ValueError) as api_error:
        # Handle deepeval API compatibility and JSON parsing errors
        error_msg = f"Deepeval evaluation error: {str(api_error)}"
        logging.error(error_msg, exc_info=True)
        allure.attach(str(api_error), name="Deepeval Error", attachment_type=allure.attachment_type.TEXT)

    except Exception as e:
        # Handle rate limits and other errors
        error_type = type(e).__name__
        error_msg = str(e)

        # Check for rate limit error (429)
        if "RateLimitError" in error_type or "429" in error_msg or "rate limit" in error_msg.lower():
            # Extract retry delay from error message (default to 5 seconds)
            import re
            delay_match = re.search(r'retry after (\d+)', error_msg)
            retry_delay = int(delay_match.group(1)) if delay_match else 5

            logging.warning(f"Rate limit exceeded. Waiting {retry_delay}s before continuing...")
            allure.attach(f"Rate limit hit. Retry after {retry_delay}s",
                          name="Rate Limit Warning",
                          attachment_type=allure.attachment_type.TEXT)

            # Wait for the suggested delay plus buffer
            time.sleep(retry_delay + REQUEST_BUFFER_SEC)

        if session_results:
            session_results["failed"] += 1

        csv_accumulator[question] = {
            "question": question,
            "ground_answer": ground_answer,
            "context": context,
            "LLM Answer": cody_response,
            "Deepeval test": "Failed",
            "gEval score": 0,
            "gEval reason": error_msg,
            "answer relevancy score": 0,
            "answer relevancy reason": "",
            "faithfulness score": 0,
            "faithfulness reason": "",
            "contextual precision score": 0,
            "contextual precision reason": "",
            "contextual recall score": 0,
            "contextual recall reason": "",
            "llm response time": llm_elapsed_ms
        }

        logging.info(f"Waiting {REQUEST_BUFFER_SEC}s before next request...")
        time.sleep(REQUEST_BUFFER_SEC)

        pytest.fail(f"Deepeval evaluation failed: {e}", pytrace=True)  # Changed from 'error' to 'e'

    except json.JSONDecodeError as json_err:
        error_msg = f"JSON parsing error: {str(json_err)}"
        logging.error(error_msg, exc_info=True)
        allure.attach(str(json_err), name="JSON Parse Error", attachment_type=allure.attachment_type.TEXT)

        if session_results:
            session_results["failed"] += 1

        csv_accumulator[question] = {
            "question": question,
            "ground_answer": ground_answer,
            "context": context,
            "LLM Answer": cody_response,
            "Deepeval test": "Failed",
            "gEval score": 0,
            "gEval reason": error_msg,
            "answer relevancy score": 0,
            "answer relevancy reason": "",
            "faithfulness score": 0,
            "faithfulness reason": "",
            "contextual precision score": 0,
            "contextual precision reason": "",
            "contextual recall score": 0,
            "contextual recall reason": "",
            "llm response time": llm_elapsed_ms
        }

        logging.info(f"Waiting {REQUEST_BUFFER_SEC}s before next request...")
        time.sleep(REQUEST_BUFFER_SEC)

        pytest.fail(f"JSON parsing error: {json_err}", pytrace=True)

    except Exception as e:
        error_msg = f"Unknown evaluation error: {str(e)}"
        logging.error(error_msg, exc_info=True)
        allure.attach(str(e), name="Evaluation Error", attachment_type=allure.attachment_type.TEXT)

        if session_results:
            session_results["failed"] += 1

        csv_accumulator[question] = {
            "question": question,
            "ground_answer": ground_answer,
            "context": context,
            "LLM Answer": cody_response,
            "Deepeval test": "Failed",
            "gEval score": 0,
            "gEval reason": error_msg,
            "answer relevancy score": 0,
            "answer relevancy reason": "",
            "faithfulness score": 0,
            "faithfulness reason": "",
            "contextual precision score": 0,
            "contextual precision reason": "",
            "contextual recall score": 0,
            "contextual recall reason": "",
            "llm response time": llm_elapsed_ms
        }

        logging.info(f"Waiting {REQUEST_BUFFER_SEC}s before next request...")
        time.sleep(REQUEST_BUFFER_SEC)

        pytest.fail(f"Deepeval evaluation failed: {e}", pytrace=True)


@pytest.mark.smoke
@pytest.mark.parametrize("row", get_test_data())
@allure.feature("cody")
@allure.story("eval")
def test_process_question_primary(row, request, csv_accumulator):
    _process_question(row, request, csv_accumulator, "normal")


@pytest.fixture(scope="session", autouse=True)
def attach_csv_fixture(request, csv_accumulator):
    yield
    try:
        if not csv_accumulator:
            logging.info("No evaluation rows to write.")
            return

        # Define columns outside the if block
        columns = [
            "question",
            "ground_answer",
            "context",
            "LLM Answer",
            "Deepeval test",
            "gEval score", "gEval reason",
            "answer relevancy score", "answer relevancy reason",
            "faithfulness score", "faithfulness reason",
            "contextual precision score", "contextual precision reason",
            "contextual recall score", "contextual recall reason",
            "llm response time"
        ]

        # Create DataFrame
        df = pd.DataFrame(list(csv_accumulator.values()))[columns]

        # Ensure output directory exists
        os.makedirs(COMMON_REPORTS_DIR, exist_ok=True)

        # Write CSV
        csv_path = os.path.join(COMMON_REPORTS_DIR, "evaluation_results.csv")
        df.to_csv(csv_path, index=False)
        logging.info(f"CSV written to {csv_path}")

        # Attach CSV to Allure
        with open(csv_path, 'r') as csv_file:
            allure.attach(csv_file.read(), name="Evaluation Data CSV", attachment_type=allure.attachment_type.CSV)

        # Write Excel
        output_path = os.path.join(COMMON_REPORTS_DIR, "deepeval_results.xlsx")
        df.to_excel(output_path, index=False, engine='openpyxl')
        logging.info(f"Excel written to {output_path}")

        # Attach Excel to Allure
        allure.attach.file(output_path, name="Evaluation Results Excel", attachment_type=allure.attachment_type.XLSX)

        logging.info("CSV and Excel files successfully attached to Allure report")

    except Exception as e:
        logging.error(f"CSV/Excel Attach Error: {e}", exc_info=True)
