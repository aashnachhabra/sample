i took reference from another app code astra attaching the code below i wanted to implement a similar flow can u compare and re analyse and suggest fixes
import allure
import pytest

from settings import ASTRA_XL
from tests.vikings.fw.common.CommonUtils import CommonUtils
from tests.vikings.fw.common.FW import FW
from ..ui_pages.AstraPage import AstraPage
from ..ui_pages.LoginPage import LoginPage
from ..ui_pages.LogoutPage import LogoutPage

COLUMNS_ORDER = [
    "actual_prompt",
    "actual_response",
    "validation_text",
    "ui_status",
    "ui_error",
    "Response time(secs)",
    "eval_threshold",
    "gEval_score",
    "gEval_reason",
    # "answer_relevancy_score",
    # "answer_relevancy_reason",
    "eval_status",
    "eval_error",
]


def test_login_astra_helper(request,page,testcase,rows_accumulator):

    request.node.COLUMNS_ORDER = COLUMNS_ORDER
    login = LoginPage(page)
    astra = AstraPage(page)
    logout = LogoutPage(page)
    try:
        CommonUtils.setupTest(testcase)
        login.login()
        astra.select_versions()
        astra.open_astra()
        ui_row=astra.execute_testcase(testcase,rows_accumulator)
        logout.logout()
        ui_eval_row=astra.evaluate_testcase(ui_row,rows_accumulator)
    except Exception as e:
        CommonUtils.handleException(testcase,rows_accumulator,e)
        raise



@pytest.mark.parametrize("testcase", FW.get_test_data(ASTRA_XL))
@pytest.mark.uieval
@allure.feature("astra")
@allure.story("ui_eval_e2e")
def test_login_astra_logout_e2e(request,page,testcase,rows_accumulator):
    test_login_astra_helper(request,page,testcase,rows_accumulator)


@pytest.mark.parametrize("testcase", FW.get_test_data(ASTRA_XL)[:1])
@pytest.mark.smoke
@allure.feature("astra")
@allure.story("ui_eval_e2e")
def test_login_astra_logout_smoke(request,page,testcase,rows_accumulator):
    test_login_astra_helper(request,page,testcase,rows_accumulator)
import logging
import traceback
from typing import Dict, Any
import allure
from tests.vikings.conftest import THRESHOLD
from tests.vikings.fw.common.CommonConstants import CommonConstants
from tests.vikings.fw.common.CommonUtils import CommonUtils
from tests.vikings.fw.common.Results import Results
from tests.vikings.fw.ui.BasePage import BasePage
from tests.vikings.fw.utils import get_multiple_scores


class AstraPage(BasePage):
    CHAT_INPUT_TEXTAREA = "altus-widget-chat-input-textarea"

    def __init__(self, page):
        self.page = page

    def select_versions(self):
        with allure.step("Toggle different Versions"):
            v1 = self.page.get_by_text("v1")
            v1.click()

            v2 = self.page.get_by_text("v2")
            v2.wait_for(state="visible")
            self.sleep(3)
            self.attach_screenshot("Toggled to v2 prompt Screenshot")

    def open_astra(self):
        with allure.step("Open Astra UI"):
            self.click_button_by_name("Astra ASTRA")

    def ask_question(self):
        with allure.step("Ask a Question"):
            question = "Give me members from group 5207564 as of 11/02/2025 without any custom attributes."
            self.get_element_by_id(self.CHAT_INPUT_TEXTAREA).click()
            self.get_element_by_id(self.CHAT_INPUT_TEXTAREA).fill(question)
            self.click_button_by_name("send")
            self.page.locator(".altus-widget-dot-elastic").wait_for(state="detached")
            self.page.locator('svg[data-icon="like"]').wait_for(state="visible")
            Results.report_event("Astra question searched successfully:" + question)
            self.attach_screenshot("Astra question search Screenshot")

    def kacey_testcase(self, testcase, rows_accumulator):
        with allure.step("Ask a Question - Kacey"):
            try:
                prompt = str(testcase.get("query", "")).strip()
                allure.dynamic.title(prompt)
                actual_response, time_taken = CommonUtils.run_prompt_and_get_response(self,prompt)
                ui_status = "passed" if actual_response and actual_response != "none" else "failed"

                result = {
                    "actual_prompt": prompt,
                    "actual_response": actual_response,
                    "ui_status": ui_status,
                    "Response time(secs)": time_taken,
                    "ui_error": ""
                }

                merged = {**testcase, **result}
                ui_row = merged
                rows_accumulator[ui_row['actual_prompt']] = ui_row
                Results.report_event("Astra question searched successfully:" + prompt)
                Results.report_event("LLM response:" + actual_response)
                self.attach_screenshot("Astra question search Screenshot")

            except Exception as e:
                logging.info(f"UI execution exception: {e}")
                allure.attach(
                    traceback.format_exc(),
                    name="UI execution exception",
                    attachment_type=allure.attachment_type.TEXT
                )
                actual_prompt_key = (
                    prompt if 'prompt' in locals() and prompt else str(testcase.get("query", "")).strip())
                result = {
                    "actual_prompt": actual_prompt_key,
                    "actual_response": "error while fetching ui response",
                    "ui_status": "failed",
                    "Response time(secs)": "0",
                    "ui_error": f"{str(e)}"
                }
                merged = {**testcase, **result}
                # test_failure_reason = f"UI execution exception: {str(e)}"

            ui_row = merged
            rows_accumulator[ui_row["actual_prompt"]] = ui_row
            Results.report_event_assert_bool(
                ui_row["ui_status"] == "passed",
                f"prompt failed: {ui_row['actual_prompt']}"
            )
            return ui_row


    def execute_testcase(self, testcase, rows_accumulator):
        with allure.step("Ask a Question"):
            try:
                prompt = str(testcase.get("prompt", "")).strip()
                mem_group_id = str(testcase.get("MemGroupID", ""))
                effective_date = str(testcase.get("EffectiveDate", ""))
                validation_text = str(testcase.get("validation_text", "")).strip()
                formatted_prompt = ( prompt.replace("$MemGroupID", mem_group_id).replace("$EffectiveDate", effective_date)).strip()
                allure.dynamic.title(formatted_prompt)
                actual_response, time_taken = CommonUtils.run_prompt_and_get_response(self, formatted_prompt)
                Results.report_event(f"Astra prompt response time:{time_taken}")

                ui_status, test_failure_reason, actual_response = CommonUtils.validate_response(actual_response, validation_text, mem_group_id)
                result = {
                    "actual_prompt": formatted_prompt,
                    "actual_response": actual_response,
                    "ui_status": ui_status,
                    "Response time(secs)": time_taken,
                    "ui_error": ""
                }
                merged = {**testcase, **result}
                Results.report_event("Astra question searched successfully:" + formatted_prompt)
                Results.report_event("validation text :" + validation_text)
                Results.report_event("LLM response:" + actual_response)
                self.attach_screenshot("Astra question search Screenshot")

            except Exception as e:
                logging.info(f"UI execution exception: {e}")
                allure.attach(traceback.format_exc(),name="UI execution exception", attachment_type=allure.attachment_type.TEXT)
                actual_prompt_key = (formatted_prompt if formatted_prompt is not None else str(testcase.get("prompt", "")).strip())
                result = {
                    "actual_prompt": actual_prompt_key,
                    "actual_response": "error while fetching ui response",
                    "ui_status": "failed",
                    "Response time(secs)": "0",
                    "ui_error": f"{str(e)}"
                }
                merged = {**testcase, **result}
                test_failure_reason = f"UI execution exception: {str(e)}"

            ui_row = merged
            rows_accumulator[ui_row['actual_prompt']] = ui_row
            Results.report_event_assert_bool( ui_row['ui_status'] == "passed",f"prompt failed: {ui_row['actual_prompt']} and more details for failure is {test_failure_reason}")
            return ui_row


    def evaluate_testcase(self, ui_row: Dict[str, Any],rows_accumulator):

        with allure.step("Evaluate Astra Testcase"):
            try:
                threshold = THRESHOLD
                if not ui_row.get("expected_response", ""):
                    ui_row["expected_response"] = ui_row["actual_response"]
                eval_payload = {
                    "question": ui_row["actual_prompt"],
                    "llmAnswer": ui_row["actual_response"],
                    "groundAnswer": ui_row["expected_response"],
                    "contextualPrecision": False,
                    "contextualRecall": False,
                    "faithfulness": False,
                    # "answerRelevancy": True,
                    "gEval": True
                }
                data = get_multiple_scores(eval_payload)
                gEval_score = data.get("gEval", {}).get("score", 0)
                gEval_reason = data.get("gEval", {}).get("reason", "")
                # answer_relevancy_score = data.get("answerRelevancy", {}).get("score", 0)
                # answer_relevancy_reason = data.get("answerRelevancy", {}).get("reason", "")

                pass_geval = gEval_score >= threshold
                # pass_answer_rel = answer_relevancy_score >= threshold

                eval_dict_list = [pass_geval]
                eval_result = {
                    "eval_threshold": threshold,
                    "gEval_score": gEval_score,
                    "gEval_reason": gEval_reason,
                    "eval_error": ""
                }
                # if not ("report" in ui_row["actual_response"].lower() and "download" in ui_row["actual_response"].lower()):
                #     eval_dict_list.append(pass_answer_rel)
                #     eval_result["answer_relevancy_score"] = answer_relevancy_score
                #     eval_result["answer_relevancy_reason"] = answer_relevancy_reason
                # else:
                #     eval_result["answer_relevancy_score"] = 0
                #     eval_result["answer_relevancy_reason"] = ""

                eval_status = "passed" if all(eval_dict_list) else "failed"
                eval_result["eval_status"] = eval_status

                merged_ui_eval = {**ui_row, **eval_result}
                # return merged_ui_eval
            except Exception as e:
                logging.info(f"Evaluation exception: {e}")
                allure.attach(traceback.format_exc(), name="Evaluation exception", attachment_type=allure.attachment_type.TEXT)
                eval_result = {
                    "eval_threshold": "",
                    "gEval_score": 0,
                    "gEval_reason": "evaluation error",
                    # "answer_relevancy_score": 0,
                    # "answer_relevancy_reason": "evaluation error",
                    "eval_status": "failed",
                    "eval_error": f"{str(e)}"
                }
                merged_ui_eval = {**ui_row, **eval_result}
                # return merged_ui_eval
            ui_eval_row=merged_ui_eval
            rows_accumulator[ui_eval_row['actual_prompt']]=ui_eval_row
            # assert ui_eval_row['eval_status'] == CommonConstants.PASSED, f"eval failed: {ui_eval_row['actual_prompt']}"
            # Results.report_event(ui_eval_row['eval_status'] == CommonConstants.PASSED,f"eval failed: {ui_eval_row['actual_prompt']}")
            return merged_ui_eval

    # python
    def evaluate_kacey_testcase(self, ui_row: Dict[str, Any], rows_accumulator):
        with allure.step("Evaluate Kacey Testcase"):
            try:
                threshold = THRESHOLD
                # Align expected field name to match non-Kacey flow
                if not ui_row.get("expected_response", ""):
                    # Prefer existing expected_answer if present, else fall back to actual_response
                    expected = ui_row.get("expected_answer", ui_row.get("actual_response", ""))
                    ui_row["expected_response"] = expected

                # Align question field to 'actual_prompt'
                question = ui_row.get("actual_prompt", ui_row.get("query", ""))

                eval_payload = {
                    "question": question,
                    "llmAnswer": ui_row.get("actual_response", ""),
                    "groundAnswer": ui_row.get("expected_response", ""),
                    "contextualPrecision": False,
                    "contextualRecall": False,
                    "faithfulness": False,
                    # "answerRelevancy": True,
                    "gEval": True
                }

                data = get_multiple_scores(eval_payload)
                gEval_score = data.get("gEval", {}).get("score", 0)
                gEval_reason = data.get("gEval", {}).get("reason", "")
                # answer_relevancy_score = data.get("answerRelevancy", {}).get("score", 0)
                # answer_relevancy_reason = data.get("answerRelevancy", {}).get("reason", "")

                pass_geval = gEval_score >= threshold
                # pass_answer_rel = answer_relevancy_score >= threshold

                eval_result = {
                    "eval_threshold": threshold,
                    "gEval_score": gEval_score,
                    "gEval_reason": gEval_reason,
                    "eval_error": ""
                }
                # if not ("report" in ui_row["actual_response"].lower() and "download" in ui_row["actual_response"].lower()):
                #     eval_dict_list.append(pass_answer_rel)
                #     eval_result["answer_relevancy_score"] = answer_relevancy_score
                #     eval_result["answer_relevancy_reason"] = answer_relevancy_reason
                # else:
                #     eval_result["answer_relevancy_score"] = 0
                #     eval_result["answer_relevancy_reason"] = ""

                eval_status = "passed" if pass_geval else "failed"
                eval_result["eval_status"] = eval_status

                merged_ui_eval = {**ui_row, **eval_result}

            except Exception as e:
                logging.info(f"Evaluation exception: {e}")
                allure.attach(
                    traceback.format_exc(),
                    name="Evaluation exception",
                    attachment_type=allure.attachment_type.TEXT
                )
                eval_result = {
                    "eval_threshold": "",
                    "gEval_score": 0,
                    "gEval_reason": "evaluation error",
                    # "answer_relevancy_score": 0,
                    # "answer_relevancy_reason": "evaluation error",
                    "eval_status": "failed",
                    "eval_error": f"{str(e)}"
                }
                merged_ui_eval = {**ui_row, **eval_result}

            # Persist into rows_accumulator using the aligned key
            key = merged_ui_eval.get("actual_prompt", merged_ui_eval.get("query", ""))
            rows_accumulator[key] = merged_ui_eval

            # Keep reporting/assertion consistent with non-Kacey flow
            Results.report_event_assert_bool(
                merged_ui_eval["eval_status"] == CommonConstants.PASSED,
                f"eval failed: {key}"
            )
            return merged_ui_eval


import io
import logging
import os

import allure
import pandas as pd
import pytest

from tests.vikings.fw.common.FW import FW

@pytest.fixture(scope="function")
def rows_accumulator(request):
    rows_dict = {}
    yield rows_dict

@pytest.fixture(scope="function", autouse=True)
def generate_result(request,rows_accumulator, tmp_path_factory):
    yield
    COLUMNS_ORDER = getattr(request.node, "COLUMNS_ORDER", [])
    if COLUMNS_ORDER:
        temp_dir = tmp_path_factory.getbasetemp()
        temp_file = temp_dir / f"{FW.get_result_file_name(os.getpid())}"
        logging.info(f"base temp:{temp_dir}")
        logging.info(f"file path:{temp_file}")
        if rows_accumulator:
            df = pd.DataFrame(list(rows_accumulator.values()))
            for col in COLUMNS_ORDER:
                if col not in df.columns:
                    df[col] = ""
            df = df[COLUMNS_ORDER]
            # Append if file exists, else write new
            if temp_file.exists():
                df.to_csv(temp_file, mode='a', header=False, index=False)
            else:
                df.to_csv(temp_file, index=False)
    else:
        logging.info("No COLUMNS_ORDER found on the test node.")

@pytest.fixture(scope="function", autouse=True)
def test_summary(request, rows_accumulator):
    yield
    try:
        COLUMNS_ORDER = getattr(request.node, "COLUMNS_ORDER", [])
        if COLUMNS_ORDER :
            if rows_accumulator:
                df = pd.DataFrame(list(rows_accumulator.values()))
                for col in COLUMNS_ORDER:
                    if col not in df.columns:
                        df[col] = ""
                df = df[COLUMNS_ORDER]
                csv_buffer = io.StringIO()
                df.to_csv(csv_buffer, index=False)
                allure.attach(csv_buffer.getvalue(), name="Test summary", attachment_type=allure.attachment_type.CSV)
        else:
            logging.info("No COLUMNS_ORDER found on the test node for summary attachment.")
    except Exception as e:
        logging.info(f"CSV Attach Error: {e}")
