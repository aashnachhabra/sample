import fitz
import re
import logging
import concurrent.futures
import orjson
from google import genai
from google.genai import types
from config import (PROJECT_ID, LOCATION, GEMINI_MODEL )

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def fix_malformed_json(json_string):
    try:
        json_string = re.sub(r'\\(?!["\\/bfnrtu])', '', json_string)
        json_string = json_string.replace('\\\\', '\\')
        json_string = re.sub(r'\\n', ' ', json_string)
        json_string = re.sub(r'\\t', ' ', json_string)
        json_string = re.sub(r'}\s*{', r'},{', json_string)
        json_string = json_string.strip()
        if not json_string.startswith("{") and not json_string.startswith("["):
            json_string = "{" + json_string
        if not json_string.endswith("}") and not json_string.endswith("]"):
            json_string = json_string + "}"
        try:
            orjson.loads(json_string)
        except orjson.JSONDecodeError as e:
            logger.warning(f"JSON still malformed after initial fixes: {e}. Attempting further corrections.")
            return json_string  # Just return the string, not a tuple
        return json_string
    except Exception as e:
        logger.error(f"Error fixing malformed JSON: {e}")
        return "{}" 

def split_pdf_by_pages_in_memory(pdf_path, pages_per_chunk=2):
    try:
        doc = fitz.open(pdf_path)
        split_chunks = []
        for i in range(0, len(doc), pages_per_chunk):
            chunk_doc = fitz.open()
            for page_num in range(i, min(i + pages_per_chunk, len(doc))):
                chunk_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
            # Save chunk to bytes in memory
            chunk_bytes = chunk_doc.write()
            if chunk_bytes:  # Only add non-empty chunks
                split_chunks.append(chunk_bytes)
        return split_chunks
    except Exception as e:
        logger.error(f"Error chunking DBS pdf: {e}")
        return []

def clean_escape_characters(data):
    if isinstance(data, dict):
        return {key.replace("\\n", " ").replace("\\t", " ").replace("\\", " "): clean_escape_characters(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [clean_escape_characters(item) for item in data]
    elif isinstance(data, str):
        return data.replace("\\n", " ").replace("\\t", " ").replace("\\", " ")
    return data

def merge_chunks_in_order(aggregated_results):
    try:
        sorted_chunks = sorted(
            [(key, value) for key, value in aggregated_results.items() if value],
            key=lambda x: int(x[0].split()[1])
        )
        final_json = {key: value for key, value in sorted_chunks}
        return final_json
    except Exception as e:
        logger.error(f"Error merging chunks in order: {e}")
        return None

def process_chunk(idx, chunk_bytes, prompt):
    chunk_key = f"chunk {idx + 1}"
    logger.info(f"Processing {chunk_key}")
    if not chunk_bytes:
        logger.error(f"No data extracted for {chunk_key}. Skipping.")
        return chunk_key, None
    json_output = process_pdf_with_gemini(chunk_bytes, prompt)
    try:
        parsed_json = orjson.loads(json_output)
        return chunk_key, parsed_json
    except orjson.JSONDecodeError as e:
        logger.error(f"Failed to parse JSON for {chunk_key}: {e}")
        logger.debug(f"Raw JSON output for {chunk_key}: {json_output}")
        try:
            fixed_json_output = fix_malformed_json(json_output)
            parsed_json = orjson.loads(fixed_json_output)
            return chunk_key, parsed_json
        except Exception as fix_error:
            logger.error(f"Failed to fix JSON for {chunk_key}: {fix_error}")
            logger.debug(f"Raw JSON output for {chunk_key}: {json_output}")
            return chunk_key, None

def process_pdf_with_gemini(chunk_bytes, prompt):
    try:
        client = genai.Client(
            vertexai=True,
            project=PROJECT_ID,
            location=LOCATION,
        )

        responses = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=[
                types.Part.from_bytes(
                    data=chunk_bytes,
                    mime_type='application/pdf',
                ),
                prompt
            ]
        )
        cleaned_responses = []
        for response in responses:
            # Extract candidates from a tuple response
            candidates = None
            if isinstance(response, tuple) and response[0] == 'candidates':
                candidates = response[1]
            else:
                candidates = [response]

            # Loop through candidates and extract JSON from the text part
            for candidate in candidates:
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        if hasattr(part, 'text') and part.text.strip().startswith("```json"):
                            raw_text = part.text
                            # Clean up code block markers and whitespace
                            cleaned_response = raw_text.strip().strip("```json").strip("```").strip()
                            cleaned_response = re.sub(r"^[^\{]+", "", cleaned_response)
                            cleaned_responses.append(cleaned_response)
                else:
                    continue
        
        output = "".join(cleaned_responses)
        sanitized_output = output.replace("\\", "\\\\")
        try:
            orjson.loads(sanitized_output)
        except orjson.JSONDecodeError:
            logger.warning("Sanitized JSON is invalid. Attempting to fix.")
            sanitized_output = fix_malformed_json(sanitized_output)
        return sanitized_output
    except Exception as e:
        logger.error(f"Error processing DBS pdf chunk with Gemini model: {e}")
        return None
    
def remove_empty_keys(data):
    """
    Recursively remove keys with value None from a dictionary.
    """
    if isinstance(data, dict):
        return {k: remove_empty_keys(v) for k, v in data.items() if v is not None}
    elif isinstance(data, list):
        return [remove_empty_keys(item) for item in data]
    else:
        return data

def process_pdf_to_json(file_path, output_file_path):

    prompt = "some text here"
    split_chunks = split_pdf_by_pages_in_memory(file_path, pages_per_chunk=2)
    logger.info(f"Split PDF into {len(split_chunks)} in-memory chunks.")

    aggregated_results = {}
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_to_chunk = {
            executor.submit(process_chunk, idx, chunk_bytes, prompt): idx
            for idx, chunk_bytes in enumerate(split_chunks)
        }
        for future in concurrent.futures.as_completed(future_to_chunk):
            chunk_key, result = future.result()
            if result is not None:
                aggregated_results[chunk_key] = result

    cleaned_results = clean_escape_characters(aggregated_results)
    cleaned_json = remove_empty_keys(cleaned_results)
    final_json = merge_chunks_in_order(cleaned_json)

    with open(output_file_path, "wb") as json_file:
        json_file.write(orjson.dumps(final_json, option=orjson.OPT_INDENT_2))
    logger.info(f"Final JSON saved in ascending order to: {output_file_path}")

    return final_json

i wanna use openai azure keys that i set in config file instead of gemini and i do not want to change any other logic just wanna use openai instead of gemini as llm

AZURE_OPENAI_API_KEY="ebc6ea027367426a9c881498b8db690f"
AZURE_OPENAI_ENDPOINT="https://kgnwl0lm6yi5ugbopenai.openai.azure.com/"
AZURE_OPENAI_MODEL_DEPLOYMENT="gpt-4-deployment"

