/Users/achhab16/Library/Caches/pypoetry/virtualenvs/app-bPuqwu3B-py3.13/bin/python3.13 "/Users/achhab16/Library/Application Support/JetBrains/IdeaIC2025.2/plugins/python-ce/helpers/pycharm/_jb_pytest_runner.py" --path /Users/achhab16/Library/Caches/pypoetry/virtualenvs/app-bPuqwu3B-py3.13/lib/python3.13/site-packages/pytest -- tests/vikings/apps/becca -m smoke --tracing off -s --alluredir allure-results --reruns 0 -n 1 --html=reports/report.html --self-contained-html
Testing started at 8:45 pm ...
Launching pytest with arguments tests/vikings/apps/becca -m smoke --tracing off -s --alluredir allure-results --reruns 0 -n 1 --html=reports/report.html --self-contained-html /Users/achhab16/Library/Caches/pypoetry/virtualenvs/app-bPuqwu3B-py3.13/lib/python3.13/site-packages/pytest --no-header --no-summary -q in /Users/achhab16/Desktop/cirrus-apps_vikings-eval

/Users/achhab16/Library/Caches/pypoetry/virtualenvs/app-bPuqwu3B-py3.13/lib/python3.13/site-packages/deepeval/__init__.py:52: UserWarning: You are using deepeval version 2.2.0, however version 3.8.4 is available. You should consider upgrading via the "pip install --upgrade deepeval" command.
  warnings.warn(
INFO 2026-02-10 20:45:06 conftest.py Loaded environment variables from .env.local
INFO 2026-02-10 20:45:07 conftest.py pytest_configure started
INFO 2026-02-10 20:45:07 conftest.py conftest is loading vars
INFO 2026-02-10 20:45:07 conftest.py Resolved app env path: /Users/achhab16/Desktop/cirrus-apps_vikings-eval/tests/vikings/apps/becca/config/.env.alpha
INFO 2026-02-10 20:45:07 conftest.py Loaded environment variables from tests/vikings/apps/becca/config/.env.alpha
INFO 2026-02-10 20:45:07 conftest.py ENV: alpha
INFO 2026-02-10 20:45:07 conftest.py APP: becca
INFO 2026-02-10 20:45:07 conftest.py RUN_ID: 1
INFO 2026-02-10 20:45:07 conftest.py SSLV: true
INFO 2026-02-10 20:45:07 conftest.py THRESHOLD: 0.5
INFO 2026-02-10 20:45:07 conftest.py pytest_configure ended
INFO 2026-02-10 20:45:07 conftest.py Session started: <Session  exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
Base temp dir init: /private/var/folders/pd/wzd_v7bx32n4byp72dqdzjkw0000gq/T/pytest-of-achhab16/pytest-85
INFO 2026-02-10 20:45:07 conftest.py Current temp dir: /private/var/folders/pd/wzd_v7bx32n4byp72dqdzjkw0000gq/T/pytest-of-achhab16/pytest-85
INFO 2026-02-10 20:45:07 conftest.py Base temp dir saved to /Users/achhab16/Desktop/cirrus-apps_vikings-eval/reports/base_temp.txt
============================= test session starts ==============================
created: 1/1 worker
/Users/achhab16/Library/Caches/pypoetry/virtualenvs/app-bPuqwu3B-py3.13/lib/python3.13/site-packages/deepeval/__init__.py:52: UserWarning: You are using deepeval version 2.2.0, however version 3.8.4 is available. You should consider upgrading via the "pip install --upgrade deepeval" command.
  warnings.warn(
INFO 2026-02-10 20:45:10 conftest.py Loaded environment variables from .env.local
INFO 2026-02-10 20:45:10 conftest.py pytest_configure started
INFO 2026-02-10 20:45:10 conftest.py conftest is loading vars
INFO 2026-02-10 20:45:10 conftest.py Resolved app env path: /Users/achhab16/Desktop/cirrus-apps_vikings-eval/tests/vikings/apps/becca/config/.env.alpha
INFO 2026-02-10 20:45:10 conftest.py Loaded environment variables from tests/vikings/apps/becca/config/.env.alpha
INFO 2026-02-10 20:45:10 conftest.py ENV: alpha
INFO 2026-02-10 20:45:10 conftest.py APP: becca
INFO 2026-02-10 20:45:10 conftest.py RUN_ID: 1
INFO 2026-02-10 20:45:10 conftest.py SSLV: true
INFO 2026-02-10 20:45:10 conftest.py THRESHOLD: 0.5
INFO 2026-02-10 20:45:10 conftest.py pytest_configure ended
INFO 2026-02-10 20:45:10 conftest.py Session started: <Session  exitstatus=<ExitCode.OK: 0> testsfailed=0 testscollected=0>
INFO 2026-02-10 20:45:10 conftest.py Current temp dir: /private/var/folders/pd/wzd_v7bx32n4byp72dqdzjkw0000gq/T/pytest-of-achhab16/pytest-85/popen-gw0
1 worker [1 item]

scheduling tests via LoadScheduling

tests/vikings/apps/becca/ui_tests/test_becca_eval_pipeline.py::test_login_becca_logout_smoke[chromium-testcase0] INFO 2026-02-10 20:46:26 conftest.py pytest_sessionfinish started
INFO 2026-02-10 20:46:26 conftest.py pytest_sessionfinish ended
Running teardown with pytest sessionfinish...
INFO 2026-02-10 20:46:27 conftest.py pytest_sessionfinish started
INFO 2026-02-10 20:46:27 conftest.py pytest_sessionfinish ended
INFO 2026-02-10 20:46:27 ReportsMerger.py Starting merge of worker files.
INFO 2026-02-10 20:46:27 FW.py worker files found for merge.
INFO 2026-02-10 20:46:27 FW.py Merged 1 files into /Users/achhab16/Desktop/cirrus-apps_vikings-eval/reports/test_results.xlsx
INFO 2026-02-10 20:46:27 ReportsMerger.py Completed merge of worker files.


=================== 1 failed, 4 warnings in 79.73s (0:01:19) ===================
INFO 2026-02-10 20:45:10 conftest.py Setup fixture before session: Eval
INFO 2026-02-10 20:45:10 conftest.py db_session started
INFO 2026-02-10 20:45:10 conftest.py Setting up database connection for session: 82af9cbd-b285-47e9-94f5-32c195e436b4
INFO 2026-02-10 20:45:10 db_utils.py Opened database connection:{'db_name': 'test_db', 'user': 'test_user', 'session_id': '82af9cbd-b285-47e9-94f5-32c195e436b4'}
INFO 2026-02-10 20:45:10 conftest.py properties_info started
INFO 2026-02-10 20:45:10 conftest.py from env info
INFO 2026-02-10 20:45:10 conftest.py ENV: alpha
INFO 2026-02-10 20:45:10 conftest.py ROLE: DEVELOPER
INFO 2026-02-10 20:45:10 conftest.py properties_info ended
INFO 2026-02-10 20:45:12 conftest.py Setup before test: test_login_becca_logout_smoke[chromium-testcase0]
INFO 2026-02-10 20:45:12 conftest.py [BECCA AUTH] Local run detected -> using storageState: artifacts/becca_storage_state.json
INFO 2026-02-10 20:46:26 conftest.py CSV Attach Error: name 'io' is not defined
INFO 2026-02-10 20:46:26 conftest.py base temp:/private/var/folders/pd/wzd_v7bx32n4byp72dqdzjkw0000gq/T/pytest-of-achhab16/pytest-85/popen-gw0
INFO 2026-02-10 20:46:26 conftest.py file path:/private/var/folders/pd/wzd_v7bx32n4byp72dqdzjkw0000gq/T/pytest-of-achhab16/pytest-85/popen-gw0/result_74078.csv
INFO 2026-02-10 20:46:26 conftest.py Teardown after test: test_login_becca_logout_smoke[chromium-testcase0]
INFO 2026-02-10 20:46:26 db_utils.py Closed database connection:{'db_name': 'test_db', 'user': 'test_user', 'session_id': '82af9cbd-b285-47e9-94f5-32c195e436b4'}
INFO 2026-02-10 20:46:26 conftest.py db_session ended
INFO 2026-02-10 20:46:26 conftest.py Teardown fixture after session: Eval

[gw0] FAILED tests/vikings/apps/becca/ui_tests/test_becca_eval_pipeline.py::test_login_becca_logout_smoke[chromium-testcase0] 
tests/vikings/apps/becca/ui_tests/test_becca_eval_pipeline.py:66 (test_login_becca_logout_smoke[chromium-testcase0])
request = <FixtureRequest for <Function test_login_becca_logout_smoke[chromium-testcase0]>>
becca_page = <Page url='https://becca-stg.prod.internal-gcp.optum.com/chat'>
testcase = {'PlanCode': 'DTKS', 'expected_answer': 'Below are the coinsurance details for Pharmaceutical Products for plan code D...e for pharmaceutical products for $PlanCode, 2024, AR, KA, INN.', 'validation_text': 'PlanCode,2024,AR,KA, In-Network'}
rows_accumulator = {'89355132': {'PlanCode': 'DTKS', 'actual_prompt': 'what is the coinsurance for pharmaceutical products for $PlanCode,...date. Please consider rephrasing your question or asking about the plan library, which might be available soon.", ...}}

    @pytest.mark.parametrize("testcase", FW.get_test_data(BECCA_XL)[:1])
    @pytest.mark.smoke
    @allure.feature("becca")
    @allure.story("ui_eval_e2e")
    def test_login_becca_logout_smoke(request, becca_page, testcase, rows_accumulator):
>       test_login_becca_helper(request, becca_page, testcase, rows_accumulator)

tests/vikings/apps/becca/ui_tests/test_becca_eval_pipeline.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/vikings/apps/becca/ui_tests/test_becca_eval_pipeline.py:38: in test_login_becca_helper
    ui_row = becca.execute_testcase(testcase, rows_accumulator)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/vikings/apps/becca/ui_pages/BeccaPage.py:570: in execute_testcase
    Results.report_event_assert_bool(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

evaluated_result = False
message = "prompt failed: what is the coinsurance for pharmaceutical products for DTKS, 2024, AR, KA, INN. | Missing tokens in ACTUAL_RESPONSE: ['DTKS', '2024', 'KA', 'In-Network']"

    def report_event_assert_bool(evaluated_result, message=None):
        if message:
>           assert evaluated_result, message
E           AssertionError: prompt failed: what is the coinsurance for pharmaceutical products for DTKS, 2024, AR, KA, INN. | Missing tokens in ACTUAL_RESPONSE: ['DTKS', '2024', 'KA', 'In-Network']
E           assert False

tests/vikings/fw/common/Results.py:62: AssertionError

Process finished with exit code 1


threw error on the smoke command
tests/vikings/apps/becca -m "smoke" --tracing off -s --alluredir allure-results --reruns 0 -n 1 --html=reports/report.html --self-contained-html
but working for uieval 
heres the code

import allure
import pytest

from settings import BECCA_XL
from tests.vikings.fw.common.CommonUtils import CommonUtils
from tests.vikings.fw.common.FW import FW

from ..ui_pages.BeccaPage import BeccaPage
from ..ui_pages.LogoutPage import LogoutPage


COLUMNS_ORDER = [
    "actual_prompt",
    "expected_response",
    "actual_response",
    "ui_status",
    "ui_error",
    "Response time(secs)",
    "eval_threshold",
    "gEval_score",
    "gEval_reason",
    "eval_status",
    "eval_error",
]
from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
import allure

def test_login_becca_helper(request, becca_page, testcase, rows_accumulator):
    request.node.COLUMNS_ORDER = COLUMNS_ORDER

    becca = BeccaPage(becca_page)
    logout = LogoutPage(becca_page)

    try:
        CommonUtils.setupTest(testcase)

        becca.open_becca()
        ui_row = becca.execute_testcase(testcase, rows_accumulator)

        if ui_row["ui_status"] == "passed":
            becca.evaluate_testcase(ui_row, rows_accumulator)

        # ✅ Logout should not fail BECCA validation
        try:
            logout.logout()
        except PlaywrightTimeoutError as e:
            # Attach debug but don't fail
            allure.attach(
                str(e),
                name="Logout skipped (Tools not found)",
                attachment_type=allure.attachment_type.TEXT
            )
            becca.attach_screenshot("Logout skipped - Tools not found")

    except Exception as e:
        CommonUtils.handleException(testcase, rows_accumulator, e)
        raise

@pytest.mark.parametrize("testcase", FW.get_test_data(BECCA_XL))
@pytest.mark.uieval
@allure.feature("becca")
@allure.story("ui_eval_e2e")
def test_login_becca_logout_e2e(request, becca_page, testcase, rows_accumulator):
    test_login_becca_helper(request, becca_page, testcase, rows_accumulator)


@pytest.mark.parametrize("testcase", FW.get_test_data(BECCA_XL)[:1])
@pytest.mark.smoke
@allure.feature("becca")
@allure.story("ui_eval_e2e")
def test_login_becca_logout_smoke(request, becca_page, testcase, rows_accumulator):
    test_login_becca_helper(request, becca_page, testcase, rows_accumulator)

# import logging
# import traceback
# import time
# import re
# from typing import Dict, Any, Tuple, List
#
# import allure
#
# from tests.vikings.apps.becca.config.Constants import Constants
# from tests.vikings.conftest import THRESHOLD
# from tests.vikings.fw.common.Results import Results
# from tests.vikings.fw.ui.BasePage import BasePage
# from tests.vikings.fw.utils import get_multiple_scores
#
#
# class BeccaPage(BasePage):
#     CHAT_INPUT_SELECTOR = ".input-section textarea[matInput]"
#     SUBMIT_BUTTON_SELECTOR = "button.submit-button"
#     START_NEW_CHAT_SELECTOR = 'button:has-text("Start New Chat")'
#     CHAT_MESSAGE_SELECTOR = "div.chat-box app-chat-message"
#
#     GENERATION_START_TIMEOUT_SECS = 10
#     GENERATION_END_TIMEOUT_SECS = 180
#
#     def __init__(self, page):
#         self.page = page
#
#     def open_becca(self):
#         with allure.step("Open Becca UI"):
#             self.page.goto(Constants.BECCA_URL, wait_until="domcontentloaded")
#
#     def start_new_chat_if_present(self):
#         with allure.step("Start New Chat (if present)"):
#             try:
#                 btn = self.page.locator(self.START_NEW_CHAT_SELECTOR)
#                 if btn.count() > 0 and btn.first.is_visible():
#                     btn.first.click()
#                     self.sleep(1)
#             except Exception:
#                 pass
#
#     def _wait_for_button_text(self, selector: str, expected_text: str, timeout_secs: int):
#         end = time.time() + timeout_secs
#         btn = self.page.locator(selector).first
#
#         while time.time() < end:
#             try:
#                 if btn.is_visible():
#                     txt = btn.inner_text().strip()
#                     if txt == expected_text:
#                         return
#             except Exception:
#                 pass
#             time.sleep(0.25)
#
#         raise TimeoutError(f"Button '{selector}' did not show text '{expected_text}' within {timeout_secs}s")
#
#     def _get_message_count(self) -> int:
#         try:
#             return self.page.locator(self.CHAT_MESSAGE_SELECTOR).count()
#         except Exception:
#             return 0
#
#     def _wait_for_message_count_increase(self, previous_count: int, timeout_secs: int = 15):
#         end = time.time() + timeout_secs
#         while time.time() < end:
#             current = self._get_message_count()
#             if current > previous_count:
#                 return
#             time.sleep(0.25)
#
#     def _get_latest_message_text(self) -> str:
#         last = self.page.locator(self.CHAT_MESSAGE_SELECTOR).last
#         last.wait_for(state="visible", timeout=30000)
#         return last.inner_text().strip()
#
#     def send_prompt_and_wait(self, prompt: str) -> str:
#         self.start_new_chat_if_present()
#         with allure.step("Send prompt in Becca and wait for response"):
#             input_box = self.page.locator(self.CHAT_INPUT_SELECTOR).first
#             submit_btn = self.page.locator(self.SUBMIT_BUTTON_SELECTOR).first
#
#             prev_count = self._get_message_count()
#
#             input_box.wait_for(state="visible", timeout=30000)
#             input_box.click()
#             input_box.fill(prompt)
#
#             submit_btn.wait_for(state="visible", timeout=30000)
#             submit_btn.click()
#
#             self._wait_for_button_text(self.SUBMIT_BUTTON_SELECTOR, "Cancel", self.GENERATION_START_TIMEOUT_SECS)
#             self._wait_for_button_text(self.SUBMIT_BUTTON_SELECTOR, "Submit", self.GENERATION_END_TIMEOUT_SECS)
#
#             self._wait_for_message_count_increase(prev_count, timeout_secs=15)
#             response_text = self._strip_ui_chrome(self._get_latest_message_text())
#             self.attach_screenshot("Becca response Screenshot")
#             return response_text
#
#     def run_prompt_and_get_response(self, prompt: str) -> Tuple[str, float]:
#         start = time.time()
#         response = self.send_prompt_and_wait(prompt)
#         return response, round(time.time() - start, 2)
#
#
#     def _normalize_for_match(self, s: str) -> str:
#         if s is None:
#             return ""
#         s = str(s).lower().replace("\n", " ").replace("\t", " ")
#         s = re.sub(r"[^\w\s%$.-]", " ", s)    # remove punctuation to stabilize matching
#         s = re.sub(r"\s+", " ", s).strip()
#         return s
#
#     def _format_prompt_with_row_values(self, prompt: str, testcase: Dict[str, Any]) -> str:
#         if not prompt:
#             return ""
#
#         def repl(match):
#             key = match.group(1)  # after $
#             if key in testcase and str(testcase.get(key, "")).strip():
#                 return str(testcase.get(key)).strip()
#
#             lower_map = {str(k).lower(): k for k in testcase.keys()}
#             if key.lower() in lower_map:
#                 real_key = lower_map[key.lower()]
#                 val = str(testcase.get(real_key, "")).strip()
#                 return val if val else match.group(0)
#
#             return match.group(0)
#
#         return re.sub(r"\$(\w+)", repl, prompt).strip()
#
#     def _validate_becca_response(
#             self,
#             actual_response: str,
#             expected_response: str,
#             validation_text: str,
#             testcase: Dict[str, Any],
#     ) -> Tuple[str, str]:
#         actual_norm = self._normalize_for_match(actual_response)
#
#         # Fail fast on empty / none-like responses
#         if not actual_response or actual_norm in {"none", ""}:
#             return "failed", "ACTUAL_RESPONSE is empty/none"
#
#         validation_text = (validation_text or "").strip()
#
#         # CASE 1: validation_text present -> token based validation
#         if validation_text:
#             raw_tokens = [t.strip() for t in validation_text.split(",") if t.strip()]
#             col_map = {str(k).strip().lower(): k for k in testcase.keys()}
#
#             resolved_tokens: List[str] = []
#             for token in raw_tokens:
#                 token_l = token.lower()
#                 if token_l in col_map:
#                     col_name = col_map[token_l]
#                     col_val = str(testcase.get(col_name, "")).strip()
#                     resolved_tokens.append(col_val if col_val else token)
#                 else:
#                     resolved_tokens.append(token)
#
#             missing = []
#             for tok in resolved_tokens:
#                 tok_norm = self._normalize_for_match(tok)
#                 if tok_norm and tok_norm not in actual_norm:
#                     missing.append(tok)
#
#             if missing:
#                 return "failed", f"Missing tokens in ACTUAL_RESPONSE: {missing}"
#             return "passed", ""
#
#         # CASE 2: no validation_text -> compare expected and actual (normalized contains)
#         expected_response = (expected_response or "").strip()
#         if expected_response:
#             exp_norm = self._normalize_for_match(expected_response)
#             if exp_norm and exp_norm in actual_norm:
#                 return "passed", ""
#             return "failed", "Expected response not found in actual response (normalized contains check)."
#
#         # CASE 3: both empty -> pass since actual is non-empty already
#         return "passed", ""
#
#
#     def execute_testcase(self, testcase: Dict[str, Any], rows_accumulator: Dict[str, Any]):
#         with allure.step("Ask a Question (Becca UI)"):
#
#             expected = str(testcase.get("expected_response", testcase.get("expected_answer", ""))).strip()
#             raw_prompt = str(testcase.get("prompt", "")).strip()
#             row_key = raw_prompt
#
#             validation_text = str(testcase.get("validation_text", "")).strip()
#
#             prompt = self._format_prompt_with_row_values(raw_prompt, testcase)
#
#             allure.dynamic.title(prompt if prompt else "Becca testcase")
#
#             test_failure_reason = ""
#             try:
#                 actual_response, time_taken = self.run_prompt_and_get_response(prompt)
#                 Results.report_event(f"Becca prompt response time: {time_taken}")
#
#                 ui_status, test_failure_reason = self._validate_becca_response(
#                     actual_response=actual_response,
#                     expected_response=expected,
#                     validation_text=validation_text,
#                     testcase=testcase
#                 )
#
#                 result = {
#                     "actual_prompt": prompt,
#                     "expected_response": expected,
#                     "actual_response": actual_response,
#                     "ui_status": ui_status,
#                     "Response time(secs)": time_taken,
#                     "ui_error": test_failure_reason or ""
#                 }
#                 merged = {**testcase, **result}
#
#                 Results.report_event("Becca question searched successfully: " + prompt)
#                 Results.report_event("validation_text: " + validation_text)
#                 Results.report_event("LLM response: " + actual_response)
#                 self.attach_screenshot("Becca question Screenshot")
#
#             except Exception as e:
#                 logging.info(f"UI execution exception: {e}")
#                 allure.attach(traceback.format_exc(), name="UI execution exception",
#                               attachment_type=allure.attachment_type.TEXT)
#                 result = {
#                     "actual_prompt": prompt,
#                     "expected_response": expected,
#                     "actual_response": "error while fetching ui response",
#                     "ui_status": "failed",
#                     "Response time(secs)": "0",
#                     "ui_error": f"{str(e)}"
#                 }
#                 merged = {**testcase, **result}
#                 test_failure_reason = f"UI execution exception: {str(e)}"
#
#             ui_row = merged
#             rows_accumulator[row_key] = ui_row
#
#             Results.report_event_assert_bool(
#                 ui_row["ui_status"] == "passed",
#                 f"prompt failed: {ui_row['actual_prompt']}" +
#                 (f" | {test_failure_reason}" if test_failure_reason else "")
#             )
#             return ui_row
#
#
#     def evaluate_testcase(self, ui_row: Dict[str, Any], rows_accumulator: Dict[str, Any]):
#         with allure.step("Evaluate Becca Testcase"):
#             try:
#                 threshold = THRESHOLD
#
#                 if not ui_row.get("expected_response", ""):
#                     ui_row["expected_response"] = ui_row.get("actual_response", "")
#
#                 eval_payload = {
#                     "question": ui_row.get("actual_prompt", ""),
#                     "llmAnswer": ui_row.get("actual_response", ""),
#                     "groundAnswer": ui_row.get("expected_response", ""),
#                     "contextualPrecision": False,
#                     "contextualRecall": False,
#                     "faithfulness": False,
#                     "gEval": True
#                 }
#
#                 data = get_multiple_scores(eval_payload)
#                 gEval_score = data.get("gEval", {}).get("score", 0)
#                 gEval_reason = data.get("gEval", {}).get("reason", "")
#
#                 pass_geval = gEval_score >= threshold
#
#                 eval_result = {
#                     "eval_threshold": threshold,
#                     "gEval_score": gEval_score,
#                     "gEval_reason": gEval_reason,
#                     "eval_error": "",
#                     "eval_status": "passed" if pass_geval else "failed"
#                 }
#
#                 merged_ui_eval = {**ui_row, **eval_result}
#
#             except Exception as e:
#                 logging.info(f"Evaluation exception: {e}")
#                 allure.attach(traceback.format_exc(), name="Evaluation exception",
#                               attachment_type=allure.attachment_type.TEXT)
#
#                 eval_result = {
#                     "eval_threshold": "",
#                     "gEval_score": 0,
#                     "gEval_reason": "evaluation error",
#                     "eval_status": "failed",
#                     "eval_error": f"{str(e)}"
#                 }
#                 merged_ui_eval = {**ui_row, **eval_result}
#
#             rows_accumulator[merged_ui_eval.get("actual_prompt", "")] = merged_ui_eval
#             return merged_ui_eval
#
#     def _strip_ui_chrome(self, text: str) -> str:
#         noise = ["content_copy", "star_border", "thumb_up_off_alt", "thumb_down_off_alt"]
#         for n in noise:
#             text = text.replace(n, "")
#         return text.strip()

import logging
import traceback
import time
import re
from typing import Dict, Any, Tuple

import allure
from tests.vikings.conftest import THRESHOLD
from tests.vikings.fw.common.Results import Results
from tests.vikings.fw.ui.BasePage import BasePage
from tests.vikings.fw.utils import get_multiple_scores


class BeccaPage(BasePage):
    # ---------------- SELECTORS ---------------- #
    CHAT_INPUT_SELECTOR = ".input-section textarea[matInput]"
    SUBMIT_BUTTON_SELECTOR = "button.submit-button"
    START_NEW_CHAT_SELECTOR = "button:has-text('Start New Chat')"
    CHAT_MESSAGE_SELECTOR = "div.chat-box app-chat-message"

    GENERATION_TIMEOUT_SECS = 180

    def __init__(self, page):
        self.page = page

    # =========================================================
    # UI HELPERS
    # =========================================================

    def open_becca(self):
        with allure.step("Open Becca UI"):
            self.page.goto(self.page.url, wait_until="domcontentloaded")

    def start_new_chat_if_present(self):
        """
        Angular-safe Start New Chat:
        - Button is conditional
        - Messages clear asynchronously
        """
        btn = self.page.locator(self.START_NEW_CHAT_SELECTOR)

        if btn.count() == 0:
            return

        try:
            btn.first.wait_for(state="visible", timeout=3000)
            btn.first.click()

            # Wait until messages are cleared
            self.page.wait_for_function(
                "document.querySelectorAll('app-chat-message').length === 0",
                timeout=5000
            )
        except Exception:
            pass

    # =========================================================
    # STREAMING-SAFE GENERATION HANDLING
    # =========================================================

    def _wait_for_generation_complete(self):
        """
        Waits for Submit -> Cancel -> Submit cycle.
        This is the ONLY reliable signal that generation finished.
        """
        self.page.wait_for_function(
            """() => {
                const btn = document.querySelector('button.submit-button');
                return btn && btn.innerText.trim() === 'Submit';
            }""",
            timeout=self.GENERATION_TIMEOUT_SECS * 1000
        )

    def _get_stable_latest_message(self, timeout_secs: int = 5) -> str:
        """
        Waits until the streamed response stops changing.
        """
        last = self.page.locator(self.CHAT_MESSAGE_SELECTOR).last
        last.wait_for(state="visible", timeout=30000)

        previous = ""
        end = time.time() + timeout_secs

        while time.time() < end:
            current = last.inner_text().strip()
            if current == previous:
                return current
            previous = current
            time.sleep(0.5)

        return previous

    def _strip_ui_chrome(self, text: str) -> str:
        noise = ["content_copy", "star_border", "thumb_up_off_alt", "thumb_down_off_alt"]
        for n in noise:
            text = text.replace(n, "")
        return text.strip()

    # =========================================================
    # PROMPT PLACEHOLDER REPLACEMENT
    # =========================================================

    def _format_prompt_with_testcase(self, prompt: str, testcase: Dict[str, Any]) -> str:
        """
        Replaces $ColumnName placeholders using testcase values.
        Example: $PlanCode -> DY82
        """
        if not prompt:
            return ""

        def replacer(match):
            key = match.group(1)
            for col, val in testcase.items():
                if col.lower() == key.lower() and str(val).strip():
                    return str(val).strip()
            return match.group(0)

        return re.sub(r"\$(\w+)", replacer, prompt).strip()

    # =========================================================
    # NORMALIZATION
    # =========================================================

    def _normalize(self, text: str) -> str:
        if not text:
            return ""
        text = text.lower()
        text = re.sub(r"[^\w\s%$.-]", " ", text)
        text = re.sub(r"\s+", " ", text)
        return text.strip()

    # =========================================================
    # SEND PROMPT
    # =========================================================

    def send_prompt_and_wait(self, prompt: str) -> str:
        self.start_new_chat_if_present()

        input_box = self.page.locator(self.CHAT_INPUT_SELECTOR).first
        submit_btn = self.page.locator(self.SUBMIT_BUTTON_SELECTOR).first

        input_box.wait_for(state="visible", timeout=30000)

        # IMPORTANT: never press Enter (Angular already binds it)
        input_box.fill("")
        input_box.fill(prompt)

        submit_btn.wait_for(state="visible", timeout=30000)
        submit_btn.click()

        self._wait_for_generation_complete()

        response = self._get_stable_latest_message()
        self.attach_screenshot("Becca response")

        return self._strip_ui_chrome(response)

    def run_prompt_and_get_response(self, prompt: str) -> Tuple[str, float]:
        start = time.time()
        response = self.send_prompt_and_wait(prompt)
        return response, round(time.time() - start, 2)

    # =========================================================
    # FINAL VALIDATION LOGIC (EXCEL CONTRACT)
    # =========================================================

    def _validate_response(
            self,
            actual_response: str,
            expected_answer: str,
            validation_text: str,
            testcase: Dict[str, Any],
    ) -> Tuple[str, str]:

        if not actual_response or self._normalize(actual_response) in {"", "none"}:
            return "failed", "ACTUAL_RESPONSE is empty"

        actual_norm = self._normalize(actual_response)

        # ---------- CASE 1: validation_text exists ----------
        if validation_text:
            tokens = [t.strip() for t in validation_text.split(",") if t.strip()]

            col_map = {
                k.lower(): str(v).strip()
                for k, v in testcase.items()
                if str(v).strip()
            }

            resolved_tokens = [
                col_map.get(tok.lower(), tok) for tok in tokens
            ]

            missing = [
                tok for tok in resolved_tokens
                if self._normalize(tok) not in actual_norm
            ]

            if missing:
                return "failed", f"Missing tokens in ACTUAL_RESPONSE: {missing}"

            return "passed", ""

        # ---------- CASE 2: expected vs actual ----------
        if expected_answer:
            expected_norm = self._normalize(expected_answer)
            if expected_norm and expected_norm in actual_norm:
                return "passed", ""
            return "failed", "Expected response not found in actual response"

        # ---------- CASE 3: nothing to validate ----------
        return "passed", ""

    # =========================================================
    # EXECUTE TESTCASE
    # =========================================================

    def execute_testcase(self, testcase: Dict[str, Any], rows_accumulator: Dict[str, Any]):
        with allure.step("Ask Question in Becca"):
            raw_prompt = str(testcase.get("prompt", "")).strip()
            expected_answer = str(testcase.get("expected_answer", "")).strip()
            validation_text = str(testcase.get("validation_text", "")).strip()

            formatted_prompt = self._format_prompt_with_testcase(raw_prompt, testcase)

            # Stable Astra-style key
            row_key = testcase.get("id") or formatted_prompt

            try:
                actual_response, time_taken = self.run_prompt_and_get_response(formatted_prompt)

                ui_status, ui_error = self._validate_response(
                    actual_response=actual_response,
                    expected_answer=expected_answer,
                    validation_text=validation_text,
                    testcase=testcase,
                )

                ui_row = {
                    **testcase,
                    "actual_prompt": formatted_prompt,
                    "expected_response": expected_answer,
                    "actual_response": actual_response,
                    "ui_status": ui_status,
                    "ui_error": ui_error,
                    "Response time(secs)": time_taken,
                }

            except Exception as e:
                logging.error(e)
                allure.attach(traceback.format_exc(), "UI exception", allure.attachment_type.TEXT)
                ui_row = {
                    **testcase,
                    "actual_prompt": formatted_prompt,
                    "actual_response": "error while fetching ui response",
                    "ui_status": "failed",
                    "ui_error": str(e),
                    "Response time(secs)": 0,
                }

            rows_accumulator[row_key] = ui_row

            Results.report_event_assert_bool(
                ui_row["ui_status"] == "passed",
                f"prompt failed: {formatted_prompt} | {ui_row['ui_error']}",
                )

            return ui_row

    # =========================================================
    # EVALUATION (ONLY IF UI PASSED)
    # =========================================================

    def evaluate_testcase(self, ui_row: Dict[str, Any], rows_accumulator: Dict[str, Any]):
        with allure.step("Evaluate Becca Testcase"):

            if ui_row["ui_status"] != "passed":
                ui_row.update({
                    "eval_status": "skipped",
                    "eval_error": "UI validation failed",
                    "eval_threshold": "",
                    "gEval_score": "",
                    "gEval_reason": "",
                })
                return ui_row

            try:
                threshold = THRESHOLD
                expected = ui_row.get("expected_response","").strip()
                payload = {
                    "question": ui_row["actual_prompt"],
                    "llmAnswer": ui_row["actual_response"],
                    "groundAnswer": expected,
                    "contextualPrecision": False,
                    "contextualRecall": False,
                    "faithfulness": False,
                    "gEval": True,
                }

                data = get_multiple_scores(payload)
                score = data.get("gEval", {}).get("score", 0)
                reason = data.get("gEval", {}).get("reason", "")

                ui_row.update({
                    "eval_threshold": threshold,
                    "gEval_score": score,
                    "gEval_reason": reason,
                    "eval_status": "passed" if score >= threshold else "failed",
                    "eval_error": "",
                })

            except Exception as e:
                allure.attach(traceback.format_exc(), "Eval exception", allure.attachment_type.TEXT)
                ui_row.update({
                    "eval_status": "failed",
                    "eval_error": str(e),
                })

            return ui_row
